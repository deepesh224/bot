import streamlit as st
from streamlit_webrtc import webrtc_streamer, WebRtcMode, RTCConfiguration
import av
import cv2
import random
import time
import torch
import google.generativeai as genai
import speech_recognition as sr
import pyttsx3
from PIL import Image
from transformers import ViltProcessor, ViltForQuestionAnswering
from datetime import datetime, timedelta
import spacy
import asyncio
import threading

# Initialize components
genai.configure(api_key="AIzaSyCK99EISZnvXHHoUYVqAmWwUWpfPOhIts0")
processor = ViltProcessor.from_pretrained("dandelin/vilt-b32-finetuned-vqa")
model = ViltForQuestionAnswering.from_pretrained("dandelin/vilt-b32-finetuned-vqa")
engine = pyttsx3.init()
nlp = spacy.load('en_core_web_sm')
recognizer = sr.Recognizer()

# Configure Google Generative AI
generation_config = {
    "temperature": 0.7,
    "top_p": 0.9,
    "top_k": 50,
    "max_output_tokens": 8192,
    "response_mime_type": "text/plain",
}

safety_settings = [
    {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
    {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
]

chat_model = genai.GenerativeModel(
    model_name="gemini-1.5-flash",
    safety_settings=safety_settings,
    generation_config=generation_config,
)

chat_session = chat_model.start_chat(history=[])

# Functions
def capture_image():
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        st.error("Error: Could not open webcam.")
        return None
    ret, frame = cap.read()
    if not ret:
        st.error("Error: Could not read frame.")
        return None
    image_path = 'captured_image.jpg'
    cv2.imwrite(image_path, frame)
    cap.release()
    return image_path

def answer_question(image_path, question):
    image = Image.open(image_path)
    encoding = processor(image, question, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**encoding)
    logits = outputs.logits
    predicted_index = logits.argmax(-1).item()
    answer = model.config.id2label[predicted_index]
    return answer

def open_application(app_name):
    applications = {
        'notepad': 'notepad.exe',
        'calculator': 'calc.exe',
        'paint': 'mspaint.exe',
        'command prompt': 'cmd.exe',
        'brave': 'brave.exe',
        'firefox': 'firefox.exe'
    }
    if app_name in applications:
        subprocess.Popen(applications[app_name])
        speak(f"Opening {app_name}")
    else:
        st.error("Sorry, I can't open that application.")

def chatbot_response(user_input):
    user_input = user_input.lower()
    responses = {
        "i love you": "I love you too guru ji!",
        "i hate you": "Why do you hate me guru ji?",
        "i'm happy": "That's great to hear guru ji!",
        "i'm sad": "I'm sorry to hear that guru ji. How can I help?",
        "i'm angry": "Take a deep breath guru ji. What's bothering you?",
        "thank you": "You're welcome guru ji!",
        "hello": "Hi guru ji! How can I help you today?",
        "goodbye": "Goodbye guru ji! Have a great day!",
        "how are you": "I'm just a bunch of code guru ji, but thanks for asking!"
    }
    return responses.get(user_input, None)

def speak(text):
    engine.say(text)
    engine.runAndWait()

def parse_reminder(text):
    doc = nlp(text)
    time_value = None
    time_unit = None
    reminder_message = None

    for ent in doc.ents:
        if ent.label_ == "TIME":
            time_value, time_unit = extract_time_value(ent.text)
        if ent.label_ == "DATE":
            time_value, time_unit = extract_time_value(ent.text)

    reminder_message = " ".join([token.text for token in doc if token.ent_type_ not in ["TIME", "DATE"]])
    if time_value and time_unit:
        reminder_time = calculate_reminder_time(time_value, time_unit)
        return reminder_time, reminder_message
    else:
        return None, None

def extract_time_value(time_text):
    time_text = time_text.lower()
    time_value = None
    time_unit = None

    if "second" in time_text:
        time_unit = "second"
        time_value = int(''.join(filter(str.isdigit, time_text)))
    elif "minute" in time_text:
        time_unit = "minute"
        time_value = int(''.join(filter(str.isdigit, time_text)))
    elif "hour" in time_text:
        time_unit = "hour"
        time_value = int(''.join(filter(str.isdigit, time_text)))

    return time_value, time_unit

def calculate_reminder_time(time_value, time_unit):
    if time_unit == "second":
        reminder_time = datetime.now() + timedelta(seconds=time_value)
    elif time_unit == "minute":
        reminder_time = datetime.now() + timedelta(minutes=time_value)
    elif time_unit == "hour":
        reminder_time = datetime.now() + timedelta(hours=time_value)
    else:
        reminder_time = None
    return reminder_time

def set_reminder(text):
    reminder_time, reminder_message = parse_reminder(text)
    if reminder_time:
        speak(f"Reminder set for {reminder_message} at {reminder_time.strftime('%H:%M:%S')}")
        while datetime.now() < reminder_time:
            time.sleep(1)
        speak(f"Reminder: {reminder_message}")
    else:
        speak("Sorry, I could not understand the time for the reminder.")

# WebRTC
RTC_CONFIGURATION = RTCConfiguration({"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]})

class AudioProcessor:
    def recv(self, frame):
        audio = frame.to_ndarray()
        recognizer.adjust_for_ambient_noise(audio)
        audio_data = sr.AudioData(audio.tobytes(), frame.sample_rate, frame.layout.name)
        try:
            text = recognizer.recognize_google(audio_data)
            st.session_state["last_speech"] = text
        except sr.UnknownValueError:
            st.session_state["last_speech"] = "Sorry, I did not understand that."
        except sr.RequestError:
            st.session_state["last_speech"] = "Could not request results; check your network connection."
        return av.AudioFrame.from_ndarray(audio, layout=frame.layout.name)

webrtc_ctx = webrtc_streamer(
    key="speech-to-text",
    mode=WebRtcMode.SENDRECV,
    rtc_configuration=RTC_CONFIGURATION,
    audio_processor_factory=AudioProcessor,
    media_stream_constraints={"audio": True, "video": False},
    async_processing=True,
)

# Streamlit UI
st.title("Virtual Assistant")

if "last_speech" not in st.session_state:
    st.session_state["last_speech"] = ""

st.write("Last Speech Input: ", st.session_state["last_speech"])

if st.button('Capture Image'):
    image_path = capture_image()
    if image_path:
        st.image(image_path, caption="Captured Image")
        question = st.text_input("Ask a question about the image")
        if st.button('Get Answer'):
            if question:
                answer = answer_question(image_path, question)
                st.write(f"Answer: {answer}")
                speak(answer)
            else:
                st.error("Please enter a question.")

user_input = st.text_input("Chat with me", value=st.session_state["last_speech"])
if st.button('Send'):
    if user_input:
        response = chatbot_response(user_input)
        if response:
            st.write(f"Chatbot: {response}")
            speak(response)
        else:
            response = chat_session.send_message(user_input)
            st.write(f"AI: {response.text}")
            speak(response.text)

app_name = st.text_input("Open Application")
if st.button('Open'):
    if app_name:
        open_application(app_name)

reminder_text = st.text_input("Set a reminder")
if st.button('Set Reminder'):
    if reminder_text:
        set_reminder(reminder_text)

st.write("Say 'chitti' to start listening for commands...")

